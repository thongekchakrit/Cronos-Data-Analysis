{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d760bc2f",
   "metadata": {},
   "source": [
    "# 1. Scrapper \n",
    "\n",
    "This notebook summarizes the codes to webscrapping of 1.cronoscan and 2.Coingecko, and uploading it to Google Bigquery for Data storage. Using tools such as python request and google app script, we will extract the relavant data and load it into Bigquery. \n",
    "\n",
    "As a free tier of google cloud platform, this project is utilizes google app script, google cloud function, bigquery and public version of Tableau to provide ETL and delivery of Statistics on Cronos CEX and DEX. To manage Google Authentication, we'll be using pydata-google-auth instead of downloading service-accounts.\n",
    "\n",
    "This notebook, also known as proto-scrapper, will be use to explore the data ingestion and summary of code to google app script. The main codes will be deployed to their respective servies.\n",
    "\n",
    "Below contains the data that we wil be scrapping: \n",
    "\n",
    "1. Information about Cronos Centralization Exchange\n",
    "- Scrap data from coingeko (API)\n",
    "    - Top 10 Exchanges\n",
    "    - Total Vol of all crypto trade\n",
    "    - ..\n",
    "\n",
    "2. Information about Cronos Decentralization Exchange\n",
    "- Download file from cronosan - Charts & Statistics (Google App Script -> Google Sheet -> Upload to BQ)\n",
    "    - The number of transaction of cronos chain\n",
    "    - Cronos Chain Unique Addresses\n",
    "    - Number of verified Contracts \n",
    "\n",
    "Link to Google App Script\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24d78f6",
   "metadata": {},
   "source": [
    "# 1.1 Data Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de8c27f",
   "metadata": {},
   "source": [
    "![image](images/data-scrapper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d175b0",
   "metadata": {},
   "source": [
    "Required python-packages\n",
    "```\n",
    "pip install requests\n",
    "pip install google-cloud-bigquery\n",
    "pip install pydata-google-auth\n",
    "pip install pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37ca46a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pydata_google_auth\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests \n",
    "import datetime\n",
    "import hashlib\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "816eef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General functions\n",
    "def bigquery_client(project_id):\n",
    "    # This function connects to bigquery client\n",
    "    credentials = pydata_google_auth.get_user_credentials(\n",
    "    ['https://www.googleapis.com/auth/cloud-platform'],\n",
    "    )\n",
    "\n",
    "    # Use the credentials in other libraries, such as the Google BigQuery\n",
    "    # client library.\n",
    "    client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "    return client\n",
    "\n",
    "def get_request(url, header):\n",
    "    # This function perform get request from site and load the response as json\n",
    "    page = requests.get(url)\n",
    "    if page.status_code == 200:\n",
    "        print(\"Successfully pulled data...\")\n",
    "        resp = json.loads(page.text)\n",
    "        return resp\n",
    "    else:\n",
    "        print(\"Unsuccessful in pulling data...\")\n",
    "        return None\n",
    "    \n",
    "def cryto_com_general(result):\n",
    "    # This function take in the request and put it into dataframe\n",
    "    # Function extracts general info about cryto.com\n",
    "    list_columns, list_values = [], []\n",
    "    ingest_at = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    for row in result:\n",
    "        if row != 'tickers':\n",
    "            list_columns = list_columns + [row]\n",
    "            list_values = list_values + [result[row]]\n",
    "    \n",
    "    list_columns = list_columns + [\"ingested_at\"]\n",
    "    list_values = list_values + [ingest_at]\n",
    "    df = pd.DataFrame(data=[list_values], columns=list_columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_crypto_com_tickers(result):\n",
    "    # This function gets the tickers from crypto.com\n",
    "    list_columns, list_values = [], []\n",
    "\n",
    "    ingest_at = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Get the columns\n",
    "    columns = [x for x in result['tickers'][0]]\n",
    "    columns = columns + ['ingested_at']\n",
    "\n",
    "    # Get the values\n",
    "    for row in result['tickers']:\n",
    "        values = []\n",
    "        for x in row:\n",
    "            values = values + [row[x]]\n",
    "        values = values + [ingest_at]\n",
    "        list_values = list_values + [values]\n",
    "\n",
    "    df = pd.DataFrame(data=list_values, columns=columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_crypto_com_tickers_historical(result):\n",
    "    # This function gets the tickers from crypto.com\n",
    "    # Get the columns\n",
    "    columns = ['timestamp', 'vol_btc', 'ingest_at']\n",
    "    ingest_at = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Get the values\n",
    "    for row in result:\n",
    "        df = df.append({\"timestamp\": row[0], \"vol_btc\":row[1], \"ingest_at\": ingest_at}, ignore_index=True)\n",
    "\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    return df\n",
    "\n",
    "\n",
    "def insert_into_bigquery(client, project_id, dataset_id, table_name, dataframe):\n",
    "    # TODO(developer): Set table_id to the ID of the table to create.\n",
    "    table_id = f\"{project_id}.{dataset_id}.{table_name}\"\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_APPEND\",\n",
    "        autodetect=True\n",
    "    )\n",
    "\n",
    "    job = client.load_table_from_dataframe(\n",
    "        dataframe, table_id, job_config=job_config\n",
    "    )  # Make an API request.\n",
    "    job.result()  # Wait for the job to complete.\n",
    "\n",
    "    table = client.get_table(table_id)  # Make an API request.\n",
    "    print(\n",
    "        \"Loaded {} rows and {} columns to {}\".format(\n",
    "            table.num_rows, len(table.schema), table_id\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "def check_table_exist(client, table_id):\n",
    "    # This function checks if home valuation csv exists\n",
    "    # TODO(developer): Set table_id to the ID of the table to determine existence.\n",
    "    # table_id = \"your-project.your_dataset.your_table\"\n",
    "    try:\n",
    "        client.get_table(table_id)  # Make an API request.\n",
    "        print(\"Table {} already exists.\".format(table_id))\n",
    "        return True\n",
    "    except:\n",
    "        print(\"Table {} is not found.\".format(table_id))\n",
    "        return False\n",
    "    \n",
    "def main(url, table_name):\n",
    "    header = \"accept: application/json\"\n",
    "    project_id = \"cryptoprojectcha\"\n",
    "    dataset_id = \"raw_coingecko\"\n",
    "    client = bigquery_client(project_id)\n",
    "    res = get_request(url, header)\n",
    "    # Extract based on table_name\n",
    "    if table_name == \"raw_cronos_details\":\n",
    "        dataframe = get_crypto_com_tickers(res)\n",
    "    elif table_name == \"raw_cronos_tickers\":\n",
    "        dataframe = get_crypto_com_tickers_historical(res)\n",
    "    else:\n",
    "        dataframe = cryto_com_general(res)\n",
    "    insert_into_bigquery(client, project_id, dataset_id, table_name, dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a823c175",
   "metadata": {},
   "source": [
    "# 2. Coingecko Scrapper\n",
    "## 2.1. Methodology\n",
    "    - Use python Request module to request data\n",
    "    - Extract and upload data into google bigquery\n",
    "    - Use Google App script to trigger cron job (daily) to updating google sheet which will \n",
    "      be consumed by \"Tableau\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0885bcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chakritthongek/Desktop/Cronos-Data-Analysis/venv/lib/python3.7/site-packages/google/cloud/bigquery/_pandas_helpers.py:472: UserWarning: Pyarrow could not determine the type of columns: status_updates.\n",
      "  \", \".join(field.name for field in unknown_type_fields)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 rows and 23 columns to cryptoprojectcha.raw_coingecko.raw_cronos_general\n",
      "Successfully pulled data...\n",
      "Loaded 546 rows and 3 columns to cryptoprojectcha.raw_coingecko.raw_cronos_tickers\n"
     ]
    }
   ],
   "source": [
    "data = {\"https://api.coingecko.com/api/v3/exchanges/crypto_com\": \"raw_cronos_details\", \"https://api.coingecko.com/api/v3/exchanges/crypto_com/volume_chart?days=2\": \"raw_cronos_tickers\",\n",
    "       \"https://api.coingecko.com/api/v3/exchanges/crypto_com\": \"raw_cronos_general\"}\n",
    "\n",
    "for key, value in data.items():\n",
    "    url = key\n",
    "    table_name = value\n",
    "    main(url, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b476f5aa",
   "metadata": {},
   "source": [
    "# 3. Cronoscan Scrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49815bb2",
   "metadata": {},
   "source": [
    "## 3.1. Methodology\n",
    "\n",
    "- Use google app script to download file directly to Google Drive\n",
    "- Trigger Google Cloud function (set up similar to webhook) to perform ETL and load data into Bigquery\n",
    "- Use Google App Script to trigger cron to update a google sheet which will be consumed by Tableau"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
